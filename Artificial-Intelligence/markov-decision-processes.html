<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    },
    TeX: { equationNumbers: { autoNumber: "AMS"} }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="../style.css">
<script src="../script.js"></script>
</head>
<body>
<main>
<h1>Markov Decision Processes</h1>
<h2>The Problem</h2>
<p>
    Consider an agent in a world. The world consists of a set of possible states, and the agent has a <em>policy</em> which tells them which action to take in any given states. So, if the current state is $s$, the policy is a function, $\pi(s)$ that takes this state and returns an action, $a$, to perform.
</p>
<p>
    However, because real life isn't a chess game, we want to relax the assumption that the current state and an action completely determine the next state. Instead, we say that given a current state $s$ and action $a$, there is a probability of transitioning to a new state $s'$. This probability is denoted $T(s,a,s')$. More formally,
    $$T(s,a,s') = P(S'=s' | S=s, A=s)$$
</p>
<p>
    We also want to relax the idea that utility is something determined just at the end of the search. Instead, we're going to earn utility after each transition, which we represent $R(s,a,s')$.
</p>
<p>
    Finally, because we want our sums of utility over time to converge, we typically want to discount utility from future actions. Due to it's nice mathematical and decision-theoretic properties, the natural choice is to exponentially discount over time. In particular, we say that a reward gained in $n$ time-steps is worth $\gamma^n$ times as much as a reward now, where $\gamma$ is a constant between 0 and 1. In this way, future rewards get assigned less value than present ones.
</p>
<p>
    Having established this framework, we can express the expected (discounted) utility the agent will achieve from a state $s$ as

    $$V^\pi(s)=\sum_{s'}{T(s,\pi(s),s')(R(s,\pi(s),s')+\gamma V^\pi(s'))}$$
</p>
<p>
    Since our goal here is typically to determine the policy, it might be more useful to write the expression where the policy is optimal:

    $$V^*(s)=\max_a{\sum_{s'}{T(s,a,s')(R(s,a,s')+\gamma V^*(s'))}}$$

    This is called the <a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a>.
</p>
<p>
    If we solve this equation by finding the value of $V^*$ for all states $s$, then we can simply use a greedy-choice algorithm to choose actions, by chosing the action that maximizes the next expected $V^*$.
</p>
<p>
    It's worth pointing out that a related equation that is sometimes also referred to as a Bellman equation is
    $$Q^*(s,a)=\sum_{s'}{T(s,a,s')(R(s,a,s')+\gamma V^*(s'))}$$
    This equation reveals the expected utility of taking action $a$ in state $s$. If you know $Q^*$, then you can also compute $V^*$, because
    $$V^*(s,a)=\max_a{Q^*(s,a)}$$
</p>
<h2>Value Iteration</h2>
<p>
    In practice, solving these equations precisely is quite difficult, so we use approximating methods like <em>value iteration</em>. While the algorithm is rather difficult to clearly and concisely state, because it consists of loops in loops in loops in loops, so we'll describe it mathematically:
</p>
<ol>
    <li>Let $V_0[s]=0$ for all states $s$.</li>
    <li>For each state, compute a new estimate for $V$ based on the old values of it's neighboring states:
    $$V_1[s]=\max_a{\sum_{s'}{T(s,a,s')(R(s,a,s')+\gamma V_0(s'))}}$$
    </li>
    <li>Compute $V_2$, $V_3$, etc. until the values in $V$ stop changing by much.</li>
</ol>
<p>
    If you want the actual code, you can find it <a href="value-iteration.py" target="_blank">here</a>.
</p>
<p>
    Although it may seem kind of ad-hoc, this algorithm's $V$'s <a href="http://uhaweb.hartford.edu/compsci/ccli/projects/QLearning.pdf">is guaranteed</a> to converge to $V^*$, which effectively gives us the optimal policy.
</p>
<p>
    To see why this is the case, we can think of value iteration as searching an Expectimax tree via iterative deepening where $V_i$ is the value assigned to the roots' children after searching $i$ layers. While the connection between iterative deepening Expectimax and value iteration takes some time to see, it should be obvious that Expectimax will yield the right answer, so this connection is the proof of value iterations' optimality.
</p>
<p>
    The only other question is why the $V$'s should converge in the first place. However, once you see the value-iteration-expectimax connection, this should also be clear, as each layer is less important than the last due to the discount factor.
</p>
<h2>Temporal-Difference Learning</h2>
<p>
    While value iteration is very effective if we know how $T$ and $R$ are defined. In real-world problems, these functions are often unknown, which motives finding an algorithm that can "explore" the world to evaluate a policy. One such algorithm is <em>Temporal-Difference (TD) Learning</em>. Here is the pseudocode
</p>
<pre>
TD_Learn(Game game, Map&lt;State,Action&gt; policy, float alpha) {
    Map&lt;State, float&gt; V;
    while(!game.is_over()) {
        s = game.current_state;
        a = policy[s];
        reward, s' = game.do(a);
        V[s] = (1-alpha) * V[s] + alpha * (reward + gamma * V[s']);
    }
    return V;
}
</pre>
<p>
    and <a href="td-learning.py" target="_blank">here</a> is the actual code.
</p>
<p>
    So, basically we just choose the action that our policy dictates and then update our estimate for <code>V[s]</code> - otherwise known as $V^\pi(s)$. Note, that while $V^*$ denotes the value of a state if we are following the optimal policy, $V^\pi$ denotes the value of a state if we're following policy $\pi$. Now, we just have to talk about why the given formula works. The key line is
</p>
<pre>
V[s] = (1-alpha) * V[s] + alpha * (reward + gamma * V[s']);
</pre>
<p>
    We want to argue that our algorithm causes <code>V[s]</code> to converge to $V^\pi(s)$ in the long-run, which is defined very similarly to $V^*$:

    $$V^\pi(s)=\sum_{s'}{T(s,\pi(s),s')(R(s,\pi(s),s')+\gamma V^\pi(s'))}$$

    Hence, $V^\pi(s)$ is just an average of $R(s,\pi(s),s')+\gamma V^\pi(s')$ weighted by $T(s,\pi(s),s')$. You should be able to convince yourself that for a small enough <code>alpha</code>, the algorithm will converge correctly.
</p>
<h2>Q-Learning</h2>
<p>
    The primary limitation of TD learning is that your policy doesn't change, which means you'll repeatedly perform bad actions. While this is good for <em>evaluating</em> policies, it is limited in <em>choosing a good policy</em>. So, it'd be nice if our policy would gradually change as we gained new information, so that we could converge to an optimal policy.
</p>
<p>
    In particular, we want to start off by exploring all our action-state combinations and then gradually start focusing on more promising paths.
</p>
<p>
    To do this, we're going to introduce a new function: $Q$. While $V$ gives us the expected value of going to a state, $Q$ is a function that gives us the expected value of performing an action in a given state:

    $$Q(s,a) = \sum_{s'}{T(s,a,s')(R(s,a,s') + \gamma V(s'))}$$

    Note, then, that
    $$V^*(s)=\max_a{Q(s,a)}$$
</p>
<p>
    Q-Learning is similar to <a href="zero-sum-games.html#monte-carlo-tree-search">Monte Carlo tree search</a> in that we randomly select actions, but with a bias towards (1) actions that ended well before and (2) actions we haven't take very often. There are many ways for us to balance these two biases that guarantee our policy will converge to the optimal policies - these are all Q-learning algorithms. Before we talk about those, let's talk about the general structure (though the code is <a href="q-learning.py" target="_blank">here</a> if you want it):
    <pre>
q_learn(Game game, float alpha, float gamma) {
    Map&lt;(State, Action), float&gt; Q;
    Map&lt;(State, Action), float&gt; N;
    while (game.current_state() != null) {
        State s = game.current_state();
        List&lt;Action&gt; actions = game.legal_actions()
        Action a = <span style="color: red">select_action</span>(s, actions, Q, N);
        ++N[s, a];
        (float, state) (reward, s') = game.do(a);
        Q[s,a]=(1-alpha)*Q[s,a]+alpha*(reward+gamma*max_q(game, Q, s'));
    }
}

max_q(Game game, Map&lt;(State, Action), float&gt; Q, State s) {
    rtn = -Infinity
    for (a in game.get_actions(s)) {
        if (Q[(s, a)] > rtn)
            rtn = Q[(s, a)]
    }
    return rtn;
}
    </pre>
</p>
<p>
    Here <code>game</code> is an object that encodes the game we're playing, <code>alpha</code> is a number between 0 and 1 representing the learning rate, and <code>gamma</code> is a number between 0 and 1 representing the discount rate. One thing to realize is that <code>max_q(game, Q, s)</code> returns our estimate of the expected utility of pursuing the optimal policy at state <code>s</code>, so <code>max_q(game, Q, s)</code> can be thought of as our estimate for $V^*(s)$.
</p>
<p>
    One simple <code style="color: red">select_action</code> function is to choose some $\epsilon$ between 0 and 1. Then, choose the action with highest expected value with probability $1-\epsilon$ and choose a random action with probability $\epsilon$. It turns out that this is sufficient to guarantee our estimates of $Q$ converge to the correct values, so let's consider this special, simple case in more detail.
</p>
<p>
    The key is the line
<pre>
Q[s,a]=(1-alpha)*Q[s,a]+alpha*(reward+gamma*max_q(game, Q, s'))
</pre>
</p>
<p>
    As with TD-learning, <code>alpha</code> causes our estimate to behave more smoothly over time. This is important for Q-learning, because actions don't always result in the same next-state, so we need some smoothing to make this algorithm remember past results and work well.
</p>
<h3>Other Exploration Functions</h3>
<p>
    Another exploration function is to choose action $a$ (in state $s$) with probability proportional to
</p>
$$Q(s,a) + \frac{k}{N(s,a)}$$
<p>
    where $k$ is some constant, and $N(s,a)$ is th number of times that action has been performed in that state. Hence, unlike the $\epsilon$-method above, this method
    <ol>
        <li>is more likely to explore poorly-explored actions</li>
        <li>changes how likely it is to explore (v. exploit) as it gains more information.</li>
    </ol>
    It thus works better than the $\epsilon$-method in practice.
</p>
<h2>Approximate Q-Learning</h2>
<p>
    If we have a very large number of state-action pairs, then regular Q-learning will converge very slowly. In this case, it is often better to use <em>approximate Q-learning</em>. Using this method, the programmer extracts features from the state-action pairs. Next, we assume that the value of taking an action, $a$, in a state, $s$, is a linear combination of these features, $f$, with weights $w$:
    $$Q(s,a) = \sum_i^n{w_i \cdot f_i(s,a)}$$
</p>
<p>
    The general psuedocode is
<pre>
Game game;
List&lt;Feature&gt; features;
float alpha;
float gamma;

float Q(s, a) {
    float rtn = 0;
    for (i = 0; i < weights.length; ++i)
        rtn += weights[i]*features[i](s, a)
    return rtn;
}

while (game.alive()) {
    weights = [0] * features.length;
    s = game.state;
    a = <span style="color: red;">choose_move</span>(game);
    reward, s' = game.do(act);
    actions = game.get_actions()
    max_val = -1e300;
    for (a' in actions) {
        val = Q(s', a')
        if (val > max_val)
            max_val = val;
    }
    diff = reward + gamma * max_val - Q(s, a)
    for (i = 0; i < weights.length; ++i)
        weights[i] += alpha * diff * features[i](s, a);
}
</pre>
</p>
<p>
    In English, we perform moves and update our weights each time. In this way, our weights (and hence our Q's) become a bit better each time. We choose our moves in a way similar to vanilla Q-learning.
</p>
<p>
    It may seem perverse that we are using our "evaluation function" to predict itself. Indeed, it naively seems like just letting all the weights be 0 accomplishes this. However, the key is that you aren't <em>just</em> predicting your evaluation, you're also predicting the reward from taking the step: reqall that the expression is not just $V(s)$, it's $R(s,a,s') + \gamma V(s')$. This reward anchors the weights to reality.
</p>
<h2>Online Learning</h2>
<p>
    Online learning is generally separated into two approaches:
    <ol>
        <li>Model-based approach</li>
        <li>Model-free approach</li>
    </ol>
</p>
<p>
    <em>Model-based learning</em> tries out actions in different states, and then tries to estimate both the transition function, $T(s,a,s')$, and also the reward function, $R(s,a,s')$. It does this by walking around and observing these values, and then using value- or policy-iteration to find the optimal policy.
</p>
<p>
    As examples:
    <ol>
        <li>Value iteration is not really a learning algorithm - it just solves the problem</li>
        <li>TD-learning is model-based</li>
        <li>Q-learning is model-free</li>
        <li>Approximate Q-learning is also model-free</li>
    </ol>
</p>
</main>
</body>
</html>
