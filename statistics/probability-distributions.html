<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
    },
    TeX: { equationNumbers: { autoNumber: "AMS"} }
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link rel="stylesheet" href="../style.css">
</head>
<body>
<main>
<h1>Probability Distributions</h1>
Numeric distributions generally fall into two types: discrete and continuous. <b>Discrete distributions</b> take integer values and <b>continuous distributions</b> take real values. Because there are an infinite number of distributions, we usually talk about <b>families</b> of distributions. For instance, if every possibility is equally likely, the distribution is a <b>uniform distribution</b>. However, there are many uniform distributions, one between 0 and 10, another between -1000 and 17. To distinguish between these different but similar distributions, we use <b>paramters</b>. For instance, the uniform distribution takes two parameters: the minimum and the maximum.
<br><br>
Speaking of uniform distributions, one thing to note is that it is impossible for a uniform distribution to have the domain $\left[-\infty,\infty\right]$, $\left[a, \infty\right]$, or $\left[\infty, b\right]$. The reason for this is that every probability must be a number between 0 and 1. In a uniform distribution, every probability must be the same number between 0 and 1. If all outcomes have a probability of 0, then all the probabilities add up to 0, but this is illegal, since the sum of all probabilities must always add up to 1. If all the outcomes have a probability that isn't 0, then the sum will be infinity, which is also illegal. However, non-uniform probabilities can have an infinite range.
<h3>Notation</h3>
Usually, capital letters represent random variables while lower-case letters represent actual numeric values. For instance, in this example
$$f(x)=P(X \le x)$$
$f(x)$ returns the probability that some randomly generated number $X$ is less than $x$. For example, let's say that $X$ is generated by the roll of a fair six-sided die. In this case $$f(3)=P(X \le 3) = \frac{3}{6} = 0.500$$
$$f(5)=P(X \le 5) = \frac{5}{6} \approx 0.833$$

<h3>Discrete Distributions</h3>
Discrete distributions are can be defined by either a <b>probability density function</b> (<b>PDF</b>) or <b>cumulative density function</b> (<b>CDF</b>) , as from one you can always derive the other. For discrete distributions, a PDF takes an integer as an input and then outputs the probability of that integer occuring. A CDF takes an integer as input and then outputs the probability that that integer or a smaller integer has of occuring. "k" is often used as the input-variable.
<br><br>
For instance, flipping two coins results in 2 heads 25% of the time, 2 tails 25% of the time, and one of each 50% of the time. So:
<br><br>
<table>
<tr><th># of heads</th><th>pdf(k)</th><th>cdf(k)</th>
<tr><td>&lt;0</td><td>0.00</td><td>0.00</td>
<tr><td>0</td><td>0.25</td><td>0.25</td>
<tr><td>1</td><td>0.50</td><td>0.75</td>
<tr><td>2</td><td>0.25</td><td>1.00</td>
<tr><td>&gt;2</td><td>0.00</td><td>1.00</td>
</table>
<br>
Because the outputs of all PDFs and CDFs are probabilities, they must always be between 0 and 1. Also, because the change in CDF is always equal to the PDF, the CDF can only increase as k increases.
<br><br>
Many distributions also have an <b>expectation</b>, which is basically the average (mean) outcome. For instance, in our coin example, we can calculate the expectation with

$$\left(0.25\right)\left(0\right)+\left(0.50\right)\left(1\right)+\left(0.25\right)\left(2\right)=1$$

In general, the expectation is defined as

$$E\left[K\right]=\sum_{k=0}^\infty{pdf\left(k\right)k}$$

Many distributions are described by their <b>variance</b>. Variance is essentially a measure of how spread out a distribution is. It is basically a measure of how far away most items are from the distribution's expected value. It is defined as

$$Var\left[K\right]=\sum_{k=0}^\infty{pdf\left(k\right)\left(k-E\left[K\right]\right)^2}$$

<br><br>

Often, it is more useful to consider the square root of a sample's variance:

$$\sigma=\sqrt{\sum_{k=0}^\infty{pdf\left(k\right)\left(k-E\left[K\right]\right)^2}}$$

This is known as the distribution's standard deviation.
<br><br>
For every probability density function, there is a cumulative probability function (CDF):

$$P(X \le x)=\sum_{i=0}^\infty{pdf(k)}$$

An example of this is given in the <b>notation</b> section.

<h3>Continuous Distributions</h3>
While discrete distributions generate integers at random, continuous distributions generate real numbers. They are usually defined using a density function such as
$$f(x)=e^{-x}, x \ge 0$$
Note, that $f(x)$ does <b>not</b> represent a probability. If you add up every real number's f(x), you will get infinity, simply because there are too many real numbers, even inside small intervals like $[0, 0.0001]$. Instead, what is really interesting is the integral of $f$:
$$F(x)=1-e^{-x}$$
Note, that F(x) has the range $[0,1]$ and always has a non-negative slope. This makes it a useful cumulative probability function. In particular, we can say that
$$P(X \lt x)=F(x), x \ge 0$$
<i>This</i> is now a kosher cumulative probability function.
<br><br>
We said before that $f(x)$ doesn't represent a valid probability density function - then why use it? Well, it turns out to be useful for finding the expected value, variance, and standard deviation of the distribution:
$$E[X]=\int_0^\infty{f(x) dx}$$
$$Var[X]=\int_0^\infty{f(x)\left(x-E[X]\right)^2 dx}$$
$$\sigma=\sqrt{\int_0^\infty{f(x)\left(x-E[X]\right)^2 dx}}$$
Note how similar these look to the discrete definitions.
</main>
</body>
</html>